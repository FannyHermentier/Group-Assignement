# -*- coding: utf-8 -*-
"""RF_Stroke

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jj6yd_78DW8DM4mCPF7iBO-anb2QWBkN

# MODEL: Predicting if there is a risk of stroke.

## Context

In a global context, stroke ranks as the second leading cause of death, contributing to around 11% of all fatalities, as reported by the World Health Organization (WHO). The dataset at hand is designed for forecasting the likelihood of a patient experiencing a stroke. It does so by analyzing input factors such as gender, age, the presence of different medical conditions, and smoking habits. Each row within the dataset furnishes essential details regarding the patient's health.

The dataset's Variables:          
1) id: unique identifier     
2) gender: "Male", "Female" or "Other"     
3) age: age of the patient     
4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension          
5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease     
6) ever_married: "No" or "Yes"            
7) work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"          
8) Residence_type: "Rural" or "Urban"              
9) avg_glucose_level: average glucose level in blood               
10) bmi: body mass index       
11) smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*           
12) stroke: 1 if the patient had a stroke or 0 if not       


*Note: "Unknown" in smoking_status means that the information is unavailable for this patient

Source of dataset: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

## O. Importing the libraries:
"""

#!pip install keras-tuner

!pip install scikit-optimize

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from skopt import BayesSearchCV
from imblearn.over_sampling import SMOTE

# Metrics that would allow us to evaluate our model
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score

#saving and exporting the model
from joblib import dump
import pickle
import joblib

"""## 1.Load the dataset


"""

from google.colab import drive
drive.mount('/content/drive')

# Specify the file path
file_path = '/content/drive/Shareddrives/MBD Term2 - Group 6/ML_Group Project/healthcare-dataset-stroke-data.csv'

# Load the data into a DataFrame
data = pd.read_csv(file_path)

data.head()

data.tail()

data.info()

"""## 2. Visualize Data:

### 2.1 Understanding the target variable:
"""

# Countplot to check class balance
sns.countplot(x='stroke', data=data)
plt.show()

"""From the distribution of the target variable we can see that the dataset is very unbalanced. Most of the dataset relates to patient that did not present a risk of stroke. As often in healthcare studies we observe more healthy clusters than unhealthy ones. Therefore, one important step will be to balance the training set in order to train properly the algorithm to identify both cases: Risk of Stroke, and No Risk of Stroke. This will be done in the 4th part when splitting the dataset into training and testing.

### 2.2: Looking into the categorical variables:
"""

# Bar charts for categorical variables
categorical_vars = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

plt.figure(figsize=(20, 12))
for i, var in enumerate(categorical_vars):
    plt.subplot(3, 3, i+1)
    sns.countplot(x=var, data=data, palette="Set2")
    plt.title(f'Distribution of {var.capitalize()}')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""Let's analyse each variable one bye one:
- **Gender:** The dataset contains more female patients than male patients, with a very small number of 'Other' gender category. This imbalance should be considered during analysis and model training to avoid bias.
- **Hypertension:** A smaller proportion of patients have hypertension. This is consistent with general health statistics, but the impact of hypertension on stroke risk could be significant.
- **Heart Disease:** The number of patients with heart disease is relatively low, similar to hypertension, which may suggest that these conditions are relatively less common or underreported in the dataset.
- **Ever Married:** The majority of patients have been married, which could correlate with the age distribution since the dataset includes adult patients.
- **Work Type:** 'Private' work type is the most common, followed by 'Self-employed' and 'Govt_job'. Few patients have 'Never_worked' or are classified as 'children'.
- **Residence Type:** The distribution between 'Rural' and 'Urban' residence types is relatively balanced, which allows for comparative analysis of stroke incidence by residence type.
- **Smoking Status:** A significant number of patients have 'never smoked', followed by 'formerly smoked' and 'smokes'. The 'Unknown' category is also sizable, indicating missing data for this variable.
"""

# Count plots for binary/categorical variables against the target variable 'stroke'
binary_categorical_vars = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

plt.figure(figsize=(20, 12))
for i, var in enumerate(binary_categorical_vars):
    plt.subplot(3, 3, i+1)
    sns.countplot(x=var, data=data, hue='stroke', palette="Set3")
    plt.title(f'Stroke by {var.capitalize()}')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""Let's also analyse one by one the impact of each variable on the target variable:
- **Gender:** Stroke incidence is shown in both male and female groups with a seemingly higher absolute number in females, likely due to the greater number of females in the dataset. The proportion within each gender would be crucial to examine to understand the risk fully.
- **Hypertension:** There's a visible difference in stroke incidence between patients with and without hypertension, with those having hypertension showing a higher incidence of stroke. This suggests hypertension is an important factor in stroke risk.
- **Heart Disease:** Patients with heart disease appear to have a higher incidence of stroke compared to those without. The presence of heart disease seems to be a significant risk factor for stroke.
- **Ever Married:** There's a noticeable number of stroke cases among those who have been married. While this may not be directly indicative of risk, it correlates with the fact that older adults are more likely to have been married and also at higher risk for stroke.
- **Work Type:** The incidence of stroke varies across work types, with 'Private' and 'Self-employed' categories showing a higher number of stroke cases. This could be related to stress levels, lifestyle factors, or simply reflect the larger numbers of people in these categories.
- **Residence Type:** There is a slight difference in stroke occurrence between rural and urban residents, with urban residents showing a slightly higher count of stroke cases. However, the difference is not as pronounced as with some other variables.
- **Smoking Status:** The distribution of stroke cases among different smoking statuses shows higher stroke incidence in the 'formerly smoked' and 'smokes' categories, which is consistent with the literature on smoking as a risk factor for stroke. The 'Unknown' category indicates missing data, which needs to be addressed.

**2.2.1:** Looking more into smoking-status variable:
"""

smoking_status_counts = data['smoking_status'].value_counts(dropna=False)
print(smoking_status_counts)

"""From these resultn we can see that the 'Unknown' category constitutes a significant portion of the dataset. Given that nearly as many entries are labeled 'Unknown' as 'never smoked', which is the most frequent known category, outright deletion of these records could result in a substantial loss of data and potential bias.
We believe it is important to treat 'Unknown' as a Separate Category for several reasons:
- **Preserves Data Integrity:** Keeps all observations in the dataset, avoiding the loss of valuable information from a significant number of records.
- **Real-World Relevance:** Recognizes the common issue of incomplete medical records, providing insights into the impact of unknown patient data on stroke risk.
- **Simplicity in Analysis:** Offers a straightforward approach without introducing potential biases from imputation assumptions, making the analysis more transparent and easier to replicate.

### 2.3: Looking into the continuous variables:
"""

# Histograms for continuous variables
continuous_vars = ['age', 'avg_glucose_level', 'bmi']

plt.figure(figsize=(15, 5))
for i, var in enumerate(continuous_vars):
    plt.subplot(1, 3, i+1)
    sns.histplot(data[var], kde=True)
    plt.title(f'Distribution of {var.capitalize()}')

plt.tight_layout()
plt.show()

"""From the above graphs here are some insights and comments we can do.
- **Age**: The age distribution is fairly uniform, indicating that the dataset includes individuals across a wide range of ages. This is positive because it allows for analysis across the age spectrum and for understanding if age is a factor in stroke incidence. Since stroke risk typically increases with age, a balanced age distribution is essential to capture this trend without bias.
- **BMI (Body Mass Index)**: The BMI distribution appears to be roughly normal with a slight right skew. This skew suggests that there is a larger portion of the dataset with individuals having a BMI above the mean. The right-tail thickness could indicate a prevalence of overweight or obesity in the sample population, or it might reflect societal trends toward higher BMIs. The presence of this skew could be important when considering risk factors for stroke, as a higher BMI is often associated with increased health risks, including the likelihood of stroke.
- **Average Glucose Level**: The distribution of average glucose levels shows a right skew, with a secondary peak on the higher end. This indicates that while most individuals have glucose levels within a "normal" range, there is a significant number of individuals with elevated glucose levels. This secondary peak might represent a subgroup within the population, such as those with prediabetes or diabetes, which are conditions related to higher stroke risk.

"""

# Box plots for continuous variables against the target variable 'stroke'
plt.figure(figsize=(15, 5))
for i, var in enumerate(continuous_vars):
    plt.subplot(1, 3, i+1)
    sns.boxplot(y=var, x='stroke', data=data, palette="Set1")
    plt.title(f'{var.capitalize()} by Stroke')

plt.tight_layout()
plt.show()

"""From these box plots we can also do several observations:
- **Age by Stroke:** There's a clear difference in the age distributions when comparing those who have had a stroke to those who have not. The median age of stroke patients is higher, and the interquartile range is shifted towards an older age. This suggests that age is a significant risk factor for strokes, which aligns with medical understanding.
- **Average Glucose Level by Stroke:** Patients who have had a stroke tend to have higher median glucose levels, with a wider interquartile range. This aligns with the known risk factor of high blood glucose for strokes. The presence of outliers, particularly on the higher glucose levels, underscores the variability within the population and could point to individuals with unmanaged or undiagnosed diabetes.

These interpretations are consistent with medical knowledge that identifies older age and higher glucose levels as risk factors for stroke. The box plots suggest that these variables may be important features for predicting the likelihood of a stroke in machine learning models.

## 3.Exploratory Data Analysis:

### 3.1: Checking for null values
"""

# Check for missing values
data.isnull().sum()

"""The null values for bmi, will be replaced with the mean of the column."""

# Calculate the mean of the 'bmi' column
bmi_mean = data['bmi'].mean()

# Replace null values with the mean
data['bmi'].fillna(bmi_mean, inplace=True)
data.info()

"""### 3.2 Correlation Matrix:"""

# Let's check the correlation between the variables
# Strong correlation between the mean radius and mean perimeter, mean area and mean primeter
plt.figure(figsize=(20,10))
sns.heatmap(data.corr(), annot=True)

"""*Interpretation/Comment:*            
From this correlation matrix, we observe that there are no strong correlation that would indicate multicolinearity between our variables and therefore would require us to delete some of them. It is important to mention, that strong correlations are considered when the correlation us superior to |0.66|.    
We observe that the *bmi* variable has a very low correlation with the target variable (0.039) which confirms the intuition from the data visualization part, that this features does not explain very well the target variable (stroke). Therefore we will not take this fetaure into account when doing our model.      
Also we note that another variable that has a very low correlation with our target variable (*stroke*) is the id which is an identifier and will be dropped.

### 3.3: Data Cleaning:
We will drop the variables we believe not important.
"""

data = data.drop(['id', 'bmi'], axis=1)

"""## 4. Train/Test Split:

### 4.1: Training and Testing split:
"""

# Split data into features (X) and target (y)
X = data.drop('stroke', axis=1)
y = data['stroke']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.info()

"""### 4.2: One-Hot-Encoding Categorical Variables:"""

# Define the categorical columns
categorical_columns = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

# Define numerical columns
numerical_columns = ['age', 'hypertension', 'heart_disease', 'avg_glucose_level']

# Create a ColumnTransformer which will apply OneHotEncoder to the categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_columns)
    ],
    remainder='passthrough'  # This will pass through other columns without transforming
)

# Fit & Transform the ColumnTransformer to the training data
X_train_transformed = preprocessor.fit_transform(X_train)

# Get the one-hot encoded column names
ohe_columns = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns)

# Concatenate with the numerical column names
all_column_names = np.concatenate((ohe_columns, numerical_columns), axis=0)

# Convert the numpy array returned by ColumnTransformer into a DataFrame
X_train_transformed_df= pd.DataFrame(X_train_transformed, columns=all_column_names)

X_train_transformed_df.info()

"""### 4.3: Balancing the training dataset:  
    
Since the minority dataset is very small, we need to use a technique to balance out the dataset. We belive that using SMOTE techhnique is the best for several reasons:
- Class Imbalance Correction: SMOTE helps to mitigate the class imbalance by generating synthetic samples for the minority class, leading to a more balanced dataset. This is particularly important in medical datasets like here, where the event of interest (stroke) is rare compared to non-events.
- Model Generalization: By creating synthetic samples, SMOTE enables the model to learn more generalized patterns rather than simply memorizing the minority class instances. This can improve model performance on unseen data.
- Overfitting Reduction: Unlike simple upsampling, which can cause overfitting by repeating minority class samples, SMOTE generates new, synthetic samples through interpolation, which helps in preventing the model from overfitting to the replicated samples.
- Algorithmic Harmony: SMOTE works well with many machine learning algorithms, especially those that are sensitive to the distribution of the target variable, like RandomForest, which is being used in this case. It provides a diverse set of examples for the algorithm to learn from.
- Improved Metrics: For imbalanced datasets, metrics like precision, recall, and F1-score are more informative than accuracy. SMOTE tends to improve these metrics for the minority class by providing a more balanced view of the class distribution, which is crucial for stroke prediction where the cost of false negatives is high.
"""

# Create an instance of SMOTE
smote = SMOTE(random_state=42)

# Fit the model and resample the data
X_train_smote, y_train_smote = smote.fit_resample(X_train_transformed, y_train)

X_train_smote.shape

y_train_smote.shape

"""## 5. Creation of the Model:

Creating a Random Forest Model with Hyperparameter tuning:
"""

# Define the Random Forest model with BayesSearchCV for hyperparameter tuning
rf_model = RandomForestClassifier(random_state=42)

param_space = {
    'n_estimators': (10, 1000),
    'max_depth': (1, 32),
    'min_samples_split': (2, 10),
    'min_samples_leaf': (1, 10)
}
opt = BayesSearchCV(
    rf_model,
    param_space,
    n_iter=32,
    cv=5,
    n_jobs=-1)

opt.fit(X_train_smote, y_train_smote)

# Return the parameters of the best model
opt.best_params_

# Make 'clf' an instance of the best model.
clf = opt.best_estimator_
clf

# Display the score obtained by the best model.
opt.best_score_

"""## 6. Make Predictions on the test dataset:

We do not balance the test dataset as it has to forecast reality!

### 6.1: One-Hot-Encoding the test set:
"""

# Transform the test data using the fitted preprocessor
X_test_transformed = preprocessor.transform(X_test)

# Get the one-hot encoded column names for categorical columns from the preprocessor
ohe_columns = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns)

# Concatenate with the numerical column names
all_column_names = np.concatenate((ohe_columns, numerical_columns), axis=0)

# Convert the numpy array returned by ColumnTransformer into a DataFrame
X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=all_column_names)
X_test_transformed_df.info()

"""### 6.2: Doing predictions:"""

# Predict the classes of the samples in the test dataset using the best model.
y_test_pred = clf.predict(X_test_transformed_df)
y_test_pred.shape

"""## 7. Construct the confusion matrix and print the classification report:"""

# Removing the seaborn visualization removes the white lines that come with it.
sns.reset_orig()

# Create a confusion matrix
ConfusionMatrixDisplay.from_predictions(
    y_test, y_test_pred,
    labels = clf.classes_,
    cmap = 'magma'
);

"""If we read this confusion matrix, it means that:
- True negatives (TN) - (0,0): The model correctly predicted 954 non-stroke cases.
- False positives (FP) - (0,1): There were 6 instances where the model incorrectly predicted a stroke.
- False negatives (FN) - (1,0): The model incorrectly predicted 60 non-stroke cases where the patients actually did have a stroke.
- True positives (TP) - (1,1): The model correctly predicted 2 stroke cases.
"""

# Print the classification report with two target names
print(classification_report(y_test, y_test_pred, target_names=['0', '1']))

"""When looking into the Classification Report we can do the following observations:
- For the non-stroke predictions (class '0'):
  - Precision is high (0.94), indicating that when the model predicts a non-stroke, it is correct 94% of the time.
  - Recall is also high (0.99), meaning the model captures 99% of all actual non-stroke cases.
  - The F1-score (0.97) is also high, showing a good balance between precision and recall for the non-stroke class.
- For the stroke predictions (class '1'):
  - Precision is low (0.22), meaning that only 22% of stroke predictions were actually correct.
  - Recall is very low (0.03), indicating the model only correctly identified 3% of all actual stroke cases.
  - The F1-score (0.06) for the stroke class is poor, reflecting the imbalance between precision and recall.
- The accuracy (0.93) of the model is high, but this metric is misleading due to the imbalanced nature of the dataset. Most of the data belongs to the non-stroke class and this is an important problem that can not be faced only by balancing out the dataset.
- The macro average F1-score (0.51) shows an average performance across both classes that is not weighted by the class imbalance, indicating that the model's performance on the minority class (stroke cases) is not as satisfactory as one would expect.
- The weighted average F1-score (0.91) takes the class imbalance into account and is therefore skewed by the model's good performance on the majority class.

In conclusion, while the model performs well in identifying non-stroke cases (class '0'), it struggles more with stroke cases (class '1'), which are of more clinical importance. The low recall for stroke cases suggests that the model would miss an important portion of actual stroke events. Therefore, despite the overall accuracy, the model needs improvement in detecting the minority classâ€”stroke cases. In order to improve the model it is fundamental to further increase the dataset with cases realted to 'stroke'.

## 8. Save and export model:
"""

# Save the mode
filename = 'rf_model.sav'
pickle.dump(opt,open(filename,'wb'))

# Save the preprocessor
preprocessor.fit(X_train)
dump(preprocessor, 'preprocessing.joblib')

